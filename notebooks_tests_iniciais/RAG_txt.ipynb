{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain langchain-cohere langchain-groq langchain-community langchain-pinecone python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WINDOWS 10\\RAG Retrieval-Augmented Generation\\RAG-Retrieval-Augmented-Generation-Aprimorando-LLMs-com-Dados-Externos\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importa√ß√£o de bibliotecas para lidar com vari√°veis de ambiente\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Loaders para carregar arquivos de texto e PDF\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Ferramentas para dividir documentos em partes menores\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Gera√ß√£o de embeddings com Cohere\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "# Integra√ß√£o com o vetorstore Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Mem√≥ria de conversa (n√£o utilizada no trecho atual, mas importada)\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# LLM da Groq (ChatGPT acelerado por hardware especializado)\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Acesso ao hub de chains pr√©-constru√≠das do LangChain\n",
    "from langchain import hub\n",
    "\n",
    "# Chains para combinar documentos e fazer busca com recupera√ß√£o\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "# Vectorstore local alternativo (FAISS), n√£o utilizado nesse c√≥digo\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaves das APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega as vari√°veis de ambiente do arquivo .env\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Obt√©m as chaves de API das vari√°veis de ambiente\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COHERE_API_KEY: TOEQPo2gzPWKLP7SUpwAW3svbK3Vwm7tZOkU6FX5\n",
      "PINECONE_API_KEY: pcsk_65d8mR_TRTfYVcecxyGRyRQPFNARgkRmit2WWY75Q4KVx3YFmmXHA3tiUKWZbsSzYsQSY5\n",
      "GROQ_API_KEY: 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\n"
     ]
    }
   ],
   "source": [
    "# Conferindo e pegou as Chaves corretamente\n",
    "print(f\"COHERE_API_KEY: {COHERE_API_KEY}\")  # Debug tempor√°rio\n",
    "print(f\"PINECONE_API_KEY: {PINECONE_API_KEY}\")  # Debug tempor√°rio\n",
    "print(f\"GROQ_API_KEY: {GROQ_API_KEY}\")  # Debug tempor√°rio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o conte√∫do do arquivo de texto\n",
    "loader = TextLoader(\"../data/mediumblog1.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de partes criadas: 4\n"
     ]
    }
   ],
   "source": [
    "# Divide o texto em partes menores para facilitar a vetoriza√ß√£o\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=1000,chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Mostra quantas partes foram geradas\n",
    "print(f\"Total de partes criadas: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporando agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia os embeddings usando o modelo da Cohere\n",
    "embeddings = CohereEmbeddings(\n",
    "    model=\"embed-english-v3.0\",\n",
    "    cohere_api_key=COHERE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome do √≠ndice no Pinecone (deve j√° existir no console do Pinecone)\n",
    "index_name = \"rag-demo\"\n",
    "\n",
    "# Cria o vetorstore a partir dos documentos, com os embeddings e configura√ß√µes do Pinecone\n",
    "vectorstore_from_docs = PineconeVectorStore.from_documents(\n",
    "    docs,\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='05cd1c2a-2322-437e-b72b-1b35a5e74f2f', metadata={'source': 'data/mediumblog1.txt'}, page_content='Title: Vector Database: What is it and Why You Should Know It?\\n\\nAuthor: Ejiro Onose\\nDate: December 22, 2023\\n\\nIf 2021 was the year of graph databases, 2023 is the year of vector databases √¢‚Ç¨‚Äù Chip Huen.\\n\\nGenerative AI and Large Language Models (LLMs) have become popular, and a vector database is one of the best tools to handle LLM data. Vector databases provide the ideal infrastructure for managing the complex, high-dimensional data that LLMs produce and rely upon.\\n\\nIn this article, I√¢‚Ç¨‚Ñ¢ll explain what vector databases are, how they work, and introduce some top vector database tools.\\n\\n What is a Vector?\\nIn machine learning (ML), a vector is a collection of numerical values that represents the features of multi-dimensional objects, such as words or images. For example, a vector representing an image might contain values related to pixel intensities and color channels.'), Document(id='6bf282e2-4c2f-4696-b7f6-cf553c0adb40', metadata={'source': 'data/mediumblog1.txt'}, page_content='Title: Vector Database: What is it and Why You Should Know It?\\n\\nAuthor: Ejiro Onose\\nDate: December 22, 2023\\n\\nIf 2021 was the year of graph databases, 2023 is the year of vector databases √¢‚Ç¨‚Äù Chip Huen.\\n\\nGenerative AI and Large Language Models (LLMs) have become popular, and a vector database is one of the best tools to handle LLM data. Vector databases provide the ideal infrastructure for managing the complex, high-dimensional data that LLMs produce and rely upon.\\n\\nIn this article, I√¢‚Ç¨‚Ñ¢ll explain what vector databases are, how they work, and introduce some top vector database tools.\\n\\n What is a Vector?\\nIn machine learning (ML), a vector is a collection of numerical values that represents the features of multi-dimensional objects, such as words or images. For example, a vector representing an image might contain values related to pixel intensities and color channels.'), Document(id='56cced7c-24a0-4efe-a500-58a098df19b2', metadata={'source': 'data/mediumblog1.txt'}, page_content='Title: Vector Database: What is it and Why You Should Know It?\\n\\nAuthor: Ejiro Onose\\nDate: December 22, 2023\\n\\nIf 2021 was the year of graph databases, 2023 is the year of vector databases √¢‚Ç¨‚Äù Chip Huen.\\n\\nGenerative AI and Large Language Models (LLMs) have become popular, and a vector database is one of the best tools to handle LLM data. Vector databases provide the ideal infrastructure for managing the complex, high-dimensional data that LLMs produce and rely upon.\\n\\nIn this article, I√¢‚Ç¨‚Ñ¢ll explain what vector databases are, how they work, and introduce some top vector database tools.\\n\\n What is a Vector?\\nIn machine learning (ML), a vector is a collection of numerical values that represents the features of multi-dimensional objects, such as words or images. For example, a vector representing an image might contain values related to pixel intensities and color channels.'), Document(id='042f501e-f059-45dd-9361-bd8ff64e153f', metadata={'source': 'data/mediumblog1.txt'}, page_content='Title: Vector Database: What is it and Why You Should Know It?\\n\\nAuthor: Ejiro Onose\\nDate: December 22, 2023\\n\\nIf 2021 was the year of graph databases, 2023 is the year of vector databases √¢‚Ç¨‚Äù Chip Huen.\\n\\nGenerative AI and Large Language Models (LLMs) have become popular, and a vector database is one of the best tools to handle LLM data. Vector databases provide the ideal infrastructure for managing the complex, high-dimensional data that LLMs produce and rely upon.\\n\\nIn this article, I√¢‚Ç¨‚Ñ¢ll explain what vector databases are, how they work, and introduce some top vector database tools.\\n\\n What is a Vector?\\nIn machine learning (ML), a vector is a collection of numerical values that represents the features of multi-dimensional objects, such as words or images. For example, a vector representing an image might contain values related to pixel intensities and color channels.')]\n"
     ]
    }
   ],
   "source": [
    "# Teste uma busca sem√¢ntica:\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "print(vectorstore.similarity_search(\"What is a vector store?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando LLM e Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o LLM da Groq com Gemma2\n",
    "llm = ChatGroq(\n",
    "    model=\"Gemma2-9b-It\",\n",
    "    groq_api_key=GROQ_API_KEY,\n",
    "    temperature=0.1\n",
    ")                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WINDOWS 10\\AppData\\Local\\Temp\\ipykernel_4796\\2577572416.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
      "c:\\Users\\WINDOWS 10\\RAG Retrieval-Augmented Generation\\RAG-Retrieval-Augmented-Generation-Aprimorando-LLMs-com-Dados-Externos\\.venv\\Lib\\site-packages\\langsmith\\client.py:278: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cria a chain de resposta com base nos documentos\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
    "\n",
    "# Cria o retriever para buscar documentos relevantes\n",
    "retriever = vectorstore_from_docs.as_retriever()\n",
    "\n",
    "# Cria a chain de RAG combinando busca + gera√ß√£o\n",
    "rag_chain = create_retrieval_chain(retriever, combine_docs_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Pergunta: O que √© React? Responda em portugu√™s.\n",
      "üí¨ Resposta: \n",
      "\n",
      "ReAct √© um novo paradigma para combinar racioc√≠nio e a√ß√£o com modelos de linguagem grandes (LLMs) para resolver tarefas de racioc√≠nio e tomada de decis√£o em linguagem. \n",
      "\n",
      "Em ess√™ncia, o ReAct incentiva o LLM a gerar tanto tra√ßos de racioc√≠nio verbal quanto a√ß√µes relacionadas a uma tarefa de forma interligada. Isso permite que o modelo realize racioc√≠nio din√¢mico para criar, manter e ajustar planos de alto n√≠vel para agir (\"raciocinar para agir\"), ao mesmo tempo em que interage com ambientes externos (como a Wikipedia) para incorporar informa√ß√µes adicionais no racioc√≠nio (\"agir para raciocinar\").\n",
      "\n",
      "\n",
      "O texto destaca algumas caracter√≠sticas importantes do ReAct:\n",
      "\n",
      "* **Intuitivo e f√°cil de projetar:** A cria√ß√£o de prompts para o ReAct √© simples, pois os anotadores humanos apenas digitam seus pensamentos em linguagem sobre as a√ß√µes que tomam.\n",
      "* **Geral e flex√≠vel:** Devido ao espa√ßo de pensamento flex√≠vel e ao formato de ocorr√™ncia de pensamento-a√ß√£o, o ReAct funciona para tarefas diversas com espa√ßos de a√ß√£o e necessidades de racioc√≠nio distintos.\n",
      "* **Eficaz e robusto:** O ReAct demonstra forte generaliza√ß√£o para novas inst√¢ncias de tarefas, superando consistentemente modelos base que utilizam apenas racioc√≠nio ou a√ß√£o.\n",
      "* **Alinhado com humanos e control√°vel:** O ReAct promete um processo de tomada de decis√£o e racioc√≠nio sequencial interpret√°vel, onde os humanos podem facilmente inspecionar o racioc√≠nio e a corre√ß√£o factual. Al√©m disso, os humanos tamb√©m podem controlar ou corrigir o comportamento do agente em tempo real por meio da edi√ß√£o de pensamentos.\n",
      "\n",
      "\n",
      "Em resumo, o ReAct √© uma abordagem promissora para integrar racioc√≠nio e a√ß√£o em LLMs, abrindo caminho para novas aplica√ß√µes em tarefas complexas que exigem compreens√£o e intera√ß√£o com o mundo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testanto LLM \n",
    "res = rag_chain.invoke({\"input\":\"O que √© React? Responda em portugu√™s.\"})\n",
    "print(\"üîç Pergunta: O que √© React? Responda em portugu√™s.\")\n",
    "print(\"üí¨ Resposta: \\n\")\n",
    "print(res['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finaliza√ß√£o com intera√ß√£o com o usu√°rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o interativa\n",
    "def fazer_pergunta_rag():\n",
    "    while True:\n",
    "        pergunta = input(\"\\n ‚ùì No que posso ajudar ‚ùì (ou digite 'sair' para encerrar):\\n> \")\n",
    "        \n",
    "        if pergunta.lower() in [\"sair\", \"exit\", \"quit\"]:\n",
    "            print(\"Encerrando. At√© logo!\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            resposta = rag_chain.invoke({\"input\":f\"{pergunta} Responda em portugu√™s\"})\n",
    "            print(\"\\nüß† Resposta:\\n\")\n",
    "            print(resposta[\"answer\"])\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Erro ao gerar resposta: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Resposta:\n",
      "\n",
      "ReAct √© um framework para tarefas de tomada de decis√£o e racioc√≠nio sequenciais, baseado em modelos de linguagem grandes. \n",
      "\n",
      "Ele funciona assim:\n",
      "\n",
      "* **Pensamento e A√ß√£o:** ReAct permite que um modelo de linguagem \"pense\" (formulando pensamentos em linguagem natural) e \"aja\" (executando a√ß√µes em um ambiente).\n",
      "* **Trajet√≥rias:** As intera√ß√µes do modelo s√£o representadas como trajet√≥rias, onde cada etapa consiste em um pensamento, uma a√ß√£o e uma observa√ß√£o resultante.\n",
      "* **Aprendizado por Exemplo:** ReAct aprende com exemplos de trajet√≥rias pr√©-definidas, chamadas de \"trajet√≥rias densas\", onde os pensamentos s√£o explicitamente fornecidos.\n",
      "* **Flexibilidade:** A estrutura flex√≠vel de ReAct permite sua aplica√ß√£o em diversas tarefas, como perguntas e respostas, verifica√ß√£o de fatos, jogos de texto e navega√ß√£o na web.\n",
      "\n",
      "**Vantagens do ReAct:**\n",
      "\n",
      "* **Intuitivo e F√°cil de Projetar:** A cria√ß√£o de prompts para ReAct √© simples, pois os humanos apenas digitam os pensamentos em linguagem natural.\n",
      "* **Geral e Flex√≠vel:** ReAct pode ser usado em uma variedade de tarefas com diferentes espa√ßos de a√ß√µes e necessidades de racioc√≠nio.\n",
      "* **Eficaz e Robusto:** ReAct demonstra desempenho consistente em tarefas de conhecimento intensivo, aprendendo com poucos exemplos e superando modelos base que apenas raciocinam ou agem.\n",
      "* **Alinhado com Humanos e Control√°vel:** ReAct permite a inspe√ß√£o do processo de racioc√≠nio e tomada de decis√£o, al√©m de permitir que os humanos controlem ou corrijam o comportamento do agente em tempo real.\n",
      "\n",
      "\n",
      "Em resumo, ReAct √© uma abordagem promissora para o racioc√≠nio e tomada de decis√£o sequencial em modelos de linguagem, combinando a capacidade de racioc√≠nio de modelos de linguagem com a flexibilidade e controle humano.\n",
      "\n",
      "Encerrando. At√© logo!\n"
     ]
    }
   ],
   "source": [
    "# Execu√ß√£o\n",
    "if __name__ == \"__main__\":\n",
    "    fazer_pergunta_rag()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
